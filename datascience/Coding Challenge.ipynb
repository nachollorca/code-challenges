{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Will it snow tomorrow?\" - The time traveler asked\n",
    "The following dataset contains climate information from over 9000 stations accross the world. The overall goal of these subtasks will be to predict whether it will snow tomorrow 14 years ago. So if today is 2023.10.27 then the weather we want to forecast is for the date 2009.10.28. You are supposed to solve the tasks using Big Query, which can be used in the Jupyter Notebook like it is shown in the following cell. For further information and how to use BigQuery in Jupyter Notebook refer to the Google Docs. \n",
    "\n",
    "The goal of this test is to test your coding knowledge in Python, BigQuery and Pandas as well as your understanding of Data Science. If you get stuck in the first part, you can use the replacement data provided in the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"../sevenlearnings-datascience-f06eb42a7306.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Task\n",
    "Change the date format to 'YYYY-MM-DD' and select the data from 2005 till 2009 for station numbers including and between 725300 and 726300 , and save it as a pandas dataframe. Note the maximum year available is 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.02s: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?query/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 377784/377784 [00:12<00:00, 29764.64rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery df\n",
    "SELECT \n",
    "    *, \n",
    "    PARSE_DATE('%Y-%m-%d', CONCAT(CAST(year AS STRING), '-', CAST(month AS STRING), '-', CAST(day AS STRING))) AS date\n",
    "FROM \n",
    "    bigquery-public-data.samples.gsod\n",
    "WHERE\n",
    "    year BETWEEN 2005 AND 2009 \n",
    "    AND station_number BETWEEN 725300 AND 726300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Task \n",
    "From here you want to work with the data from all stations 725300 to 725330 that have information from 2005 till 2009. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a first analysis of the remaining dataset, clean or drop data depending on how you see appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nacho -** for a quick exploration and decide on what to drop/transform I:\n",
    " - Check if the schema brings some docs/descriptions of the columns (it does not)\n",
    " - Use `pd.describe()` to have a feeling of the values of both numerical and categorical/bool cols\n",
    " - Check the ratio of uniques and nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 246.64query/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.67s/rows]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>ordinal_position</th>\n",
       "      <th>is_nullable</th>\n",
       "      <th>data_type</th>\n",
       "      <th>is_generated</th>\n",
       "      <th>generation_expression</th>\n",
       "      <th>is_stored</th>\n",
       "      <th>is_hidden</th>\n",
       "      <th>is_updatable</th>\n",
       "      <th>is_system_defined</th>\n",
       "      <th>is_partitioning_column</th>\n",
       "      <th>clustering_ordinal_position</th>\n",
       "      <th>collation_name</th>\n",
       "      <th>column_default</th>\n",
       "      <th>rounding_mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bigquery-public-data</td>\n",
       "      <td>samples</td>\n",
       "      <td>gsod</td>\n",
       "      <td>station_number</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>INT64</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          table_catalog table_schema table_name     column_name  \\\n",
       "0  bigquery-public-data      samples       gsod  station_number   \n",
       "\n",
       "   ordinal_position is_nullable data_type is_generated generation_expression  \\\n",
       "0                 1          NO     INT64        NEVER                  None   \n",
       "\n",
       "  is_stored is_hidden is_updatable is_system_defined is_partitioning_column  \\\n",
       "0      None        NO         None                NO                     NO   \n",
       "\n",
       "   clustering_ordinal_position collation_name column_default rounding_mode  \n",
       "0                          NaN           NULL           NULL          None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "-- looking for column descriptions\n",
    "\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    bigquery-public-data.samples.INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE\n",
    "    table_name=\"gsod\"\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniques</th>\n",
       "      <th>nulls</th>\n",
       "      <th>null_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>station_number</th>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wban_number</th>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_temp</th>\n",
       "      <td>1139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mean_temp_samples</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_dew_point</th>\n",
       "      <td>1050</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mean_dew_point_samples</th>\n",
       "      <td>22</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_sealevel_pressure</th>\n",
       "      <td>648</td>\n",
       "      <td>145847</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mean_sealevel_pressure_samples</th>\n",
       "      <td>22</td>\n",
       "      <td>145847</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_station_pressure</th>\n",
       "      <td>904</td>\n",
       "      <td>369069</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mean_station_pressure_samples</th>\n",
       "      <td>22</td>\n",
       "      <td>369069</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_visibility</th>\n",
       "      <td>716</td>\n",
       "      <td>9679</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mean_visibility_samples</th>\n",
       "      <td>22</td>\n",
       "      <td>9679</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_wind_speed</th>\n",
       "      <td>629</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mean_wind_speed_samples</th>\n",
       "      <td>22</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_sustained_wind_speed</th>\n",
       "      <td>109</td>\n",
       "      <td>1092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_gust_wind_speed</th>\n",
       "      <td>106</td>\n",
       "      <td>130899</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_temperature</th>\n",
       "      <td>551</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_temperature_explicit</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_temperature</th>\n",
       "      <td>1</td>\n",
       "      <td>377784</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_temperature_explicit</th>\n",
       "      <td>1</td>\n",
       "      <td>377784</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_precipitation</th>\n",
       "      <td>424</td>\n",
       "      <td>45344</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow_depth</th>\n",
       "      <td>59</td>\n",
       "      <td>371089</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fog</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hail</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thunder</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tornado</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>1824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    uniques   nulls  null_percentage\n",
       "station_number                          217       0                0\n",
       "wban_number                             204       0                0\n",
       "year                                      5       0                0\n",
       "month                                    12       0                0\n",
       "day                                      31       0                0\n",
       "mean_temp                              1139       0                0\n",
       "num_mean_temp_samples                    21       0                0\n",
       "mean_dew_point                         1050     158                0\n",
       "num_mean_dew_point_samples               22     158                0\n",
       "mean_sealevel_pressure                  648  145847               39\n",
       "num_mean_sealevel_pressure_samples       22  145847               39\n",
       "mean_station_pressure                   904  369069               98\n",
       "num_mean_station_pressure_samples        22  369069               98\n",
       "mean_visibility                         716    9679                3\n",
       "num_mean_visibility_samples              22    9679                3\n",
       "mean_wind_speed                         629     360                0\n",
       "num_mean_wind_speed_samples              22     360                0\n",
       "max_sustained_wind_speed                109    1092                0\n",
       "max_gust_wind_speed                     106  130899               35\n",
       "max_temperature                         551      98                0\n",
       "max_temperature_explicit                  3      98                0\n",
       "min_temperature                           1  377784              100\n",
       "min_temperature_explicit                  1  377784              100\n",
       "total_precipitation                     424   45344               12\n",
       "snow_depth                               59  371089               98\n",
       "fog                                       2       0                0\n",
       "rain                                      2       0                0\n",
       "snow                                      2       0                0\n",
       "hail                                      2       0                0\n",
       "thunder                                   2       0                0\n",
       "tornado                                   2       0                0\n",
       "date                                   1824       0                0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# overviewing features (I don't print it for space)\n",
    "df.describe(include=np.number).T\n",
    "df.describe(exclude=np.number).T\n",
    "\n",
    "\n",
    "# checking uniques and nulls\n",
    "unique_and_nulls = {}\n",
    "\n",
    "for col in df:\n",
    "    uniques = len(df[col].unique())\n",
    "    nulls = df[col].isnull().sum()\n",
    "    null_percentage = round((nulls / len(df)) * 100)\n",
    "\n",
    "    unique_and_nulls[col] = {\n",
    "        \"uniques\": uniques,\n",
    "        \"nulls\": nulls,\n",
    "        \"null_percentage\": null_percentage\n",
    "    }\n",
    "\n",
    "pd.DataFrame(unique_and_nulls).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nacho -** at first glance:\n",
    " - We don't need year, month and date because that info is passed in the new date\n",
    " - There are several columns filled >95% with nulls we can drop\n",
    " - Presumably the number of samples used for the means are not necessary either\n",
    " - Looks like wban_number is not usable. Arguably station_number could hold some geographical info. However, it looks almost as a categorical value (although encoded as a number) and I will drop it too. Theres another public table in BQ with station info, so I could eventually take latitude, longitude and elevation with something like the query below (I won't do it):\n",
    "\n",
    "```SQL\n",
    "with nb_gsod as(\n",
    "  SELECT \n",
    "    *,\n",
    "    CAST(station_number as string) as usaf,\n",
    "    PARSE_DATE('%Y-%m-%d', CONCAT(CAST(year AS STRING), '-', CAST(month AS STRING), '-', CAST(day AS STRING))) AS date\n",
    "  FROM \n",
    "    bigquery-public-data.samples.gsod\n",
    "  WHERE\n",
    "    year BETWEEN 2005 AND 2009 \n",
    "    AND station_number BETWEEN 725300 AND 726300\n",
    "),\n",
    "\n",
    "stations as (\n",
    "  select usaf, lat, lon, elev\n",
    "  from `bigquery-public-data.noaa_gsod.stations`\n",
    ")\n",
    "\n",
    "select *\n",
    "from\n",
    "  nb_gsod\n",
    "  left join stations on nb_gsod.usaf=stations.usaf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    columns=[\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"min_temperature\",\n",
    "        \"mean_station_pressure\",\n",
    "        \"num_mean_station_pressure_samples\",\n",
    "        \"num_mean_temp_samples\",\n",
    "        \"num_mean_dew_point_samples\",\n",
    "        \"num_mean_sealevel_pressure_samples\",\n",
    "        \"num_mean_visibility_samples\",\n",
    "        \"min_temperature_explicit\",\n",
    "        \"snow_depth\", # this could stay\n",
    "        \"num_mean_wind_speed_samples\",\n",
    "        \"wban_number\",\n",
    "        \"station_number\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Nacho** - with the cleaner table I can start transforming:\n",
    " - Encoding bools (fog, rain, etc.) - I'd go for easy binary encoding\n",
    " - Encoding dates - I'll use values 1 to 365 and apply cosine encoding\n",
    " - Treating remaining mulls - Looks like only 2% of the rows have >2 nulls (out of 14 features). I'd assume Normal distribution and impute the mean for numerical and mode for booleans (after train/test split to avoid leakage).\n",
    " - Outlier analysis and imputation could also be done, but I don't think is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rows with >0 nulls: 0.6463561188403956\n",
      "% of rows with >1 nulls: 0.2118670986595515\n",
      "% of rows with >2 nulls: 0.02373843254346399\n",
      "% of rows with >3 nulls: 0.0009105732376172628\n",
      "% of rows with >4 nulls: 8.470448722021049e-05\n",
      "% of rows with >5 nulls: 1.0588060902526311e-05\n",
      "% of rows with >6 nulls: 1.0588060902526311e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>mean_dew_point</th>\n",
       "      <th>mean_sealevel_pressure</th>\n",
       "      <th>mean_visibility</th>\n",
       "      <th>mean_wind_speed</th>\n",
       "      <th>max_sustained_wind_speed</th>\n",
       "      <th>max_gust_wind_speed</th>\n",
       "      <th>max_temperature</th>\n",
       "      <th>max_temperature_explicit</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>fog</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>hail</th>\n",
       "      <th>thunder</th>\n",
       "      <th>tornado</th>\n",
       "      <th>cyclic_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>294254</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>37.799999</td>\n",
       "      <td>1011.500000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.1</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>44.099998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44644</th>\n",
       "      <td>35.299999</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>1016.265921</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>25.1</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.444322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277482</th>\n",
       "      <td>56.099998</td>\n",
       "      <td>53.200001</td>\n",
       "      <td>1018.299988</td>\n",
       "      <td>8.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.515039</td>\n",
       "      <td>53.599998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46800</th>\n",
       "      <td>22.400000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1016.265921</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>15.9</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219082</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>1016.265921</td>\n",
       "      <td>13.4</td>\n",
       "      <td>11.9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.400002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean_temp  mean_dew_point  mean_sealevel_pressure  mean_visibility  \\\n",
       "294254  64.000000       37.799999             1011.500000             10.0   \n",
       "44644   35.299999       21.700001             1016.265921             10.0   \n",
       "277482  56.099998       53.200001             1018.299988              8.6   \n",
       "46800   22.400000       18.000000             1016.265921              8.7   \n",
       "219082  42.000000       31.500000             1016.265921             13.4   \n",
       "\n",
       "        mean_wind_speed  max_sustained_wind_speed  max_gust_wind_speed  \\\n",
       "294254              8.4                      18.1            28.900000   \n",
       "44644               9.3                      25.1            29.900000   \n",
       "277482              3.1                       8.0            23.515039   \n",
       "46800               4.3                      15.9            22.900000   \n",
       "219082             11.9                      26.0            35.000000   \n",
       "\n",
       "        max_temperature  max_temperature_explicit  total_precipitation  fog  \\\n",
       "294254        44.099998                       0.0               0.0000    0   \n",
       "44644         26.600000                       1.0               0.0667    0   \n",
       "277482        53.599998                       1.0               0.0400    0   \n",
       "46800         19.400000                       1.0               0.0000    0   \n",
       "219082        37.400002                       1.0               0.0667    0   \n",
       "\n",
       "        rain  snow  hail  thunder  tornado  cyclic_date  \n",
       "294254     0     0     0        0        0     0.135986  \n",
       "44644      0     0     0        0        0     0.444322  \n",
       "277482     0     0     0        0        0     0.026363  \n",
       "46800      0     0     0        0        0     0.983514  \n",
       "219082     0     0     0        0        0     0.265012  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary encode bools\n",
    "bool_cols = [col for col in df.columns if df[col].dtype == bool]\n",
    "bool_cols.append(\"max_temperature_explicit\") # was object type but filled with bools\n",
    "\n",
    "for col in bool_cols:\n",
    "    df[col] = df[col].replace({True:1, False:0})\n",
    "    \n",
    "\n",
    "# cyclic encode date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    \n",
    "def encode_cyclical(day: int) -> float:\n",
    "    return (np.cos(2 * np.pi * day / 366) + 1) / 2\n",
    "\n",
    "df['cyclic_date'] = df.apply(lambda row: encode_cyclical(row['day_of_year']), axis=1)\n",
    "\n",
    "\n",
    "# checking rows with >x nulls\n",
    "null_rows = df.isnull().sum(axis=1)\n",
    "for i in range(7):\n",
    "    print(f\"% of rows with >{i} nulls: {len(null_rows[null_rows > i])/len(df)}\")\n",
    "    \n",
    "\n",
    "# split data before imputation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_day = \"2009-11-04\"\n",
    "test = df[df['date'] == test_day]\n",
    "df = df[df['date'] < test_day] # I should not have data after \"tomorrow\"\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=42) \n",
    "\n",
    "\n",
    "# I don't need not-encoded dates anymore\n",
    "for split in [train, val, test]:\n",
    "    split.drop(columns=[\"date\", \"day_of_year\"], inplace=True)\n",
    "    \n",
    "\n",
    "# imput training set statistics\n",
    "train_modes = train[bool_cols].mode().iloc[0]\n",
    "num_cols = [col for col in df.columns if df[col].dtype == float]\n",
    "train_means = train[num_cols].mean()\n",
    "\n",
    "for split in [train, val, test]:\n",
    "    for col in split.columns:\n",
    "        if col in bool_cols:\n",
    "            split[col].fillna(train_modes[col], inplace=True)\n",
    "        elif col in num_cols:\n",
    "            split[col].fillna(train_means[col], inplace=True)\n",
    "            \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nacho** - looking good now. Finally I:\n",
    " - Analyze correlation - drop three cols above 0.8\n",
    " - Scale the features - I fit a standard scaler on the training set and transform all splits with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove correlated cols\n",
    "corr_matrix = train[num_cols].corr()\n",
    "threshold = 0.8\n",
    "\n",
    "mask = np.ones(corr_matrix.shape)\n",
    "mask = np.triu(mask, k=1).astype(bool)\n",
    "upper = corr_matrix.where(mask)\n",
    "correlated_cols = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "for split in [train, val, test]:\n",
    "    split.drop(columns = correlated_cols, inplace=True)\n",
    "    \n",
    "\n",
    "# grab binary target and scale the rest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y_train = train[\"snow\"]\n",
    "y_test = test[\"snow\"]\n",
    "y_val = val[\"snow\"]\n",
    "\n",
    "x_train = train.drop('snow', axis=1) \n",
    "x_test = test.drop('snow', axis=1)\n",
    "x_val = val.drop('snow', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_train.columns, index=x_test.index)\n",
    "x_val = pd.DataFrame(scaler.transform(x_val), columns=x_train.columns, index=x_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Task\n",
    "Now it is time to split the data, into a training, evaluation and test set. As a reminder, the date we are trying to predict snow fall for should constitute your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009-11-06'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "str(dt.datetime.today()- dt.timedelta(days=14*365)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nacho -** I made it above, before null imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "If you made it up to here all by yourself, you can use your prepared dataset to train an algorithm of your choice to forecast whether it will snow on the following date for each station in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009-11-06'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "str(dt.datetime.today()- dt.timedelta(days=14*365)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are allowed to use any library you are comfortable with such as sklearn, tensorflow, keras etc. \n",
    "If you did not manage to finish part one feel free to use the data provided in 'coding_challenge.csv' Note that this data does not represent a solution to Part 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nacho** - a simple random forest achieves perfect scores 🤷. Same for a shallow neural network on just 2 epochs. Since most rows have value `False` for the target variable, I'll go with F1score as main metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true: list, y_pred: list) -> dict:\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "test_preds = rf.predict(x_test)\n",
    "compute_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9145/9145 [==============================] - 27s 3ms/step - loss: 0.0055 - precision: 0.9986 - recall: 0.9959 - val_loss: 3.0659e-06 - val_precision: 1.0000 - val_recall: 1.0000\n",
      "Epoch 2/2\n",
      "9145/9145 [==============================] - 26s 3ms/step - loss: 6.3238e-07 - precision: 1.0000 - recall: 1.0000 - val_loss: 2.6459e-08 - val_precision: 1.0000 - val_recall: 1.0000\n",
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Dense(32, activation='relu', input_dim=x_train.shape[1]))\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
    "\n",
    "nn.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "test_preds = nn.predict(x_test)\n",
    "test_preds = [1 if i > 0.5 else 0 for i in test_preds]\n",
    "compute_metrics(y_test, test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
